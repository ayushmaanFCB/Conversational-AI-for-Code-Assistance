{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATING BLEU SCORES FOR THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and the vocabulary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from prompt_model import Encoder, EncoderLayer, PositionwiseFeedforwardLayer, MultiHeadAttentionLayer, Decoder, DecoderLayer, Seq2Seq\n",
    "\n",
    "model_path = 'models/conversational-ai-model-cpu.pt'\n",
    "model = torch.load(model_path)\n",
    "\n",
    "with open('./vocabs/source_vocab.pkl', 'rb') as f:\n",
    "        src_vocab = pickle.load(f)\n",
    "\n",
    "with open('./vocabs/target_vocab.pkl', 'rb') as f:\n",
    "        trg_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "SRC = Field(tokenize=lambda x: x.split(),\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "\n",
    "TRG = Field(tokenize=lambda x: x.split(),\n",
    "            init_token='<sos>',\n",
    "            eos_token='<eos>',\n",
    "            lower=True)\n",
    "SRC.vocab = src_vocab\n",
    "TRG.vocab = trg_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Function to convert the prompts and replies to Torch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50000):\n",
    "    model.eval()\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(\n",
    "                trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import untokenize\n",
    "\n",
    "def eng_to_python(src):\n",
    "    src = src.split(\" \")\n",
    "    translation, _ = translate_sentence(src, SRC, TRG, model, device)\n",
    "    return untokenize(translation[:-1]).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Preparing our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "original_replies = []\n",
    "\n",
    "with open(\"data/test.txt\", \"r\") as file:\n",
    "    current_prompt = None\n",
    "    current_reply = []\n",
    "\n",
    "    for line in file:\n",
    "        if line.startswith(\"#\"):\n",
    "            if current_prompt is not None:\n",
    "                original_replies.append(''.join(current_reply).strip())\n",
    "\n",
    "            current_prompt = line.strip(\"# \").strip()\n",
    "            prompts.append(current_prompt)\n",
    "            current_reply = []\n",
    "        else:\n",
    "            current_reply.append(line)\n",
    "    \n",
    "    if current_prompt is not None:\n",
    "        original_replies.append(''.join(current_reply).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_reply = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    bot_reply.append(eng_to_python(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Generating the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for reply pair 1: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 2: 1.1200407237786664e-231\n",
      "BLEU score for reply pair 3: 1.3091834502273125e-231\n",
      "BLEU score for reply pair 4: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 5: 1.2183324802375697e-231\n",
      "BLEU score for reply pair 6: 8.844844403089352e-232\n",
      "BLEU score for reply pair 7: 6.373704167435469e-155\n",
      "BLEU score for reply pair 8: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 9: 7.601159375410181e-232\n",
      "BLEU score for reply pair 10: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 11: 1.0518351895246305e-231\n",
      "BLEU score for reply pair 12: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 13: 1.154647032204335e-231\n",
      "BLEU score for reply pair 14: 1.0832677820940877e-231\n",
      "BLEU score for reply pair 15: 1.2508498911928379e-231\n",
      "BLEU score for reply pair 16: 1.2183324802375697e-231\n",
      "BLEU score for reply pair 17: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 18: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 19: 1.2882297539194154e-231\n",
      "BLEU score for reply pair 20: 1.0669733992029681e-231\n"
     ]
    }
   ],
   "source": [
    "bleu_scores = []\n",
    "\n",
    "for original, bot in zip(original_replies, bot_reply):\n",
    "    reference = [original.split()]  \n",
    "    candidate = bot.split() \n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    bleu_scores.append(score)\n",
    "\n",
    "for i, score in enumerate(bleu_scores):\n",
    "    print(f\"BLEU score for reply pair {i+1}: {round(score, 1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
