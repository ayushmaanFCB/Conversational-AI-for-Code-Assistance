{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: tqdm in e:\\conversational-ai-for-code-assistance\\.venv\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: click in e:\\conversational-ai-for-code-assistance\\.venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: colorama in e:\\conversational-ai-for-code-assistance\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.4.2 nltk-3.8.1 regex-2024.5.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 24.1.2 is available.\n",
      "You should consider upgrading via the 'E:\\Conversational-AI-for-Code-Assistance\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.987727354491445e-155"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "hypothesis = ['This', 'is', 'cat'] \n",
    "reference = ['This', 'is', 'a', 'cat']\n",
    "references = [reference] # list of references for 1 sentence.\n",
    "list_of_references = [references] # list of references for all sentences in corpus.\n",
    "list_of_hypotheses = [hypothesis] # list of hypotheses that corresponds to list of references.\n",
    "nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Function for nth Fibonacci number\",\n",
    "    \"Write a Python program to add two lists of the same length.\",\n",
    "    \"Write a Python program to add numbers from two lists if the first list item is even and the second list item is odd.\",\n",
    "    \"Write a Python function that returns True if a string is empty and False otherwise.\",\n",
    "    \"Write a Python function that accepts a measurement value in inches and returns the equivalent in feet.\",\n",
    "    \"Write a Python function that takes the age and returns the age in days.\",\n",
    "    \"Write a Python program to count the number of each vowel.\",\n",
    "    \"Write a Python program to print two sets union using operations like in mathematics.\",\n",
    "    \"Write a Python program to print two sets intersection using operations like in mathematics.\",\n",
    "    \"Write a Python program to print two sets differences using operations like in mathematics.\",\n",
    "    \"Write a Python program to print two sets symmetric differences using operations like in mathematics.\"\n",
    "]\n",
    "\n",
    "reference_codes = [\n",
    "    \"\"\"\n",
    "    def Fibonacci(n): \n",
    "        if n<0: \n",
    "            print(\"Incorrect input\") \n",
    "        elif n==0: \n",
    "            return 0\n",
    "        elif n==1: \n",
    "            return 1\n",
    "        else: \n",
    "            return Fibonacci(n-1) + Fibonacci(n-2) \n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def add_two_list_items():\n",
    "        num1 = [1,2,3]\n",
    "        num2 = [4,5,6]\n",
    "        sum = num1 + num2\n",
    "        print(f'Sum: {sum}')\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def add_two_lists_even_odd(l1, l2):\n",
    "        new = []\n",
    "        for x, y in zip(l1, l2):\n",
    "            if l1%2 == 0 and l2%2 != 0:\n",
    "                new.append(x+y)\n",
    "        return new\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def is_empty(s):\n",
    "        if s == \"\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def inches_to_feet(inches):\n",
    "        if inches < 12:\n",
    "            return 0\n",
    "        return inches / 12\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def calc_age(age):\n",
    "        calculation = age * 365\n",
    "        return calculation\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    vowels = 'aeiou'\n",
    "    ip_str = 'Hello, have you tried our tutorial section yet?'\n",
    "    ip_str = ip_str.casefold()\n",
    "    count = {}.fromkeys(vowels,0)\n",
    "    for char in ip_str:\n",
    "        if char in count:\n",
    "            count[char] += 1\n",
    "    print(count)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Union of E and N is\", E | N)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Intersection of E and N is\", E & N)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Difference of E and N is\", E - N)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Symmetric difference of E and N is\", E ^ N)\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Example generated codes from your model\n",
    "generated_codes = [\n",
    "    \"\"\"\n",
    "    def Fibonacci(n):\n",
    "        if n < 0:\n",
    "            return \"Incorrect input\"\n",
    "        elif n == 0:\n",
    "            return 0\n",
    "        elif n == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return Fibonacci(n-1) + Fibonacci(n-2)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def add_two_list_items():\n",
    "        num1 = [1, 2, 3]\n",
    "        num2 = [4, 5, 6]\n",
    "        result = [x + y for x, y in zip(num1, num2)]\n",
    "        print(f'Sum: {result}')\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def add_two_lists_even_odd(l1, l2):\n",
    "        new_list = []\n",
    "        for x, y in zip(l1, l2):\n",
    "            if x % 2 == 0 and y % 2 != 0:\n",
    "                new_list.append(x + y)\n",
    "        return new_list\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def is_empty(s):\n",
    "        return s == \"\"\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def inches_to_feet(inches):\n",
    "        return inches / 12\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def calc_age(age):\n",
    "        return age * 365\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    vowels = 'aeiou'\n",
    "    ip_str = 'Hello, have you tried our tutorial section yet?'.lower()\n",
    "    count = {v: ip_str.count(v) for v in vowels}\n",
    "    print(count)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Union of E and N is\", E.union(N))\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Intersection of E and N is\", E.intersection(N))\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Difference of E and N is\", E.difference(N))\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    E = {0, 2, 4, 6, 8}\n",
    "    N = {1, 2, 3, 4, 5}\n",
    "    print(\"Symmetric difference of E and N is\", E.symmetric_difference(N))\n",
    "    \"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_code(code):\n",
    "    return code.strip().split()\n",
    "\n",
    "tokenized_references = [tokenize_code(code) for code in reference_codes]\n",
    "tokenized_generated = [tokenize_code(code) for code in generated_codes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: def Fibonacci(n): if n<0: print(\"Incorrect input\") elif n==0: return 0 elif n==1: return 1 else: return Fibonacci(n-1) + Fibonacci(n-2)\n",
      "Generated: def Fibonacci(n): if n < 0: return \"Incorrect input\" elif n == 0: return 0 elif n == 1: return 1 else: return Fibonacci(n-1) + Fibonacci(n-2)\n",
      "BLEU score: 0.3233\n",
      "\n",
      "Reference: def add_two_list_items(): num1 = [1,2,3] num2 = [4,5,6] sum = num1 + num2 print(f'Sum: {sum}')\n",
      "Generated: def add_two_list_items(): num1 = [1, 2, 3] num2 = [4, 5, 6] result = [x + y for x, y in zip(num1, num2)] print(f'Sum: {result}')\n",
      "BLEU score: 0.1241\n",
      "\n",
      "Reference: def add_two_lists_even_odd(l1, l2): new = [] for x, y in zip(l1, l2): if l1%2 == 0 and l2%2 != 0: new.append(x+y) return new\n",
      "Generated: def add_two_lists_even_odd(l1, l2): new_list = [] for x, y in zip(l1, l2): if x % 2 == 0 and y % 2 != 0: new_list.append(x + y) return new_list\n",
      "BLEU score: 0.3859\n",
      "\n",
      "Reference: def is_empty(s): if s == \"\": return True else: return False\n",
      "Generated: def is_empty(s): return s == \"\"\n",
      "BLEU score: 0.0632\n",
      "\n",
      "Reference: def inches_to_feet(inches): if inches < 12: return 0 return inches / 12\n",
      "Generated: def inches_to_feet(inches): return inches / 12\n",
      "BLEU score: 0.2223\n",
      "\n",
      "Reference: def calc_age(age): calculation = age * 365 return calculation\n",
      "Generated: def calc_age(age): return age * 365\n",
      "BLEU score: 0.1866\n",
      "\n",
      "Reference: vowels = 'aeiou' ip_str = 'Hello, have you tried our tutorial section yet?' ip_str = ip_str.casefold() count = {}.fromkeys(vowels,0) for char in ip_str: if char in count: count[char] += 1 print(count)\n",
      "Generated: vowels = 'aeiou' ip_str = 'Hello, have you tried our tutorial section yet?'.lower() count = {v: ip_str.count(v) for v in vowels} print(count)\n",
      "BLEU score: 0.3777\n",
      "\n",
      "Reference: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Union of E and N is\", E | N)\n",
      "Generated: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Union of E and N is\", E.union(N))\n",
      "BLEU score: 0.8624\n",
      "\n",
      "Reference: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Intersection of E and N is\", E & N)\n",
      "Generated: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Intersection of E and N is\", E.intersection(N))\n",
      "BLEU score: 0.8624\n",
      "\n",
      "Reference: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Difference of E and N is\", E - N)\n",
      "Generated: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Difference of E and N is\", E.difference(N))\n",
      "BLEU score: 0.8624\n",
      "\n",
      "Reference: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Symmetric difference of E and N is\", E ^ N)\n",
      "Generated: E = {0, 2, 4, 6, 8} N = {1, 2, 3, 4, 5} print(\"Symmetric difference of E and N is\", E.symmetric_difference(N))\n",
      "BLEU score: 0.8684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for ref, hyp in zip(tokenized_references, tokenized_generated):\n",
    "    score = sentence_bleu([ref], hyp, smoothing_function=smoothie)\n",
    "    print(f\"Reference: {' '.join(ref)}\")\n",
    "    print(f\"Generated: {' '.join(hyp)}\")\n",
    "    print(f\"BLEU score: {score:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranslate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleu_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_bleu, SmoothingFunction\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# def load_model(path):\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/conversational-ai-model-gpu.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#     model.eval()  # Set model to evaluation mode\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#     return model\u001b[39;00m\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1026\u001b[0m                      map_location,\n\u001b[0;32m   1027\u001b[0m                      pickle_module,\n\u001b[0;32m   1028\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1029\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1451\u001b[0m )\n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:265\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 265\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    267\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32me:\\Conversational-AI-for-Code-Assistance\\.venv\\lib\\site-packages\\torch\\serialization.py:249\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    246\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    250\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    254\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "# def load_model(path):\n",
    "model = torch.load('./models/conversational-ai-model-gpu.pt')\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     return model\n",
    "\n",
    "model.eval() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a function to load your trained model and generate code\n",
    "# For demonstration purposes, this function is a placeholder\n",
    "def generate_code_from_question(question):\n",
    "    # Your code generation logic here\n",
    "    # This is just an example\n",
    "    generated_code = \"def example_function(): pass\"  # Replace with actual model output\n",
    "    return generated_code\n",
    "\n",
    "# Define your reference codes for evaluation\n",
    "reference_codes = {\n",
    "    \"Function for nth Fibonacci number\": \"def Fibonacci(n): if n<0: print('Incorrect input') elif n==0: return 0 elif n==1: return 1 else: return Fibonacci(n-1) + Fibonacci(n-2)\",\n",
    "    \"Write a Python program to add two lists of the same length.\": \"def add_two_list_items(): num1 = [1,2,3] num2 = [4,5,6] sum = num1 + num2 print(f'Sum: {sum}')\",\n",
    "    # Add other reference codes here\n",
    "}\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu_score(reference_code, generated_code):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    # Tokenize the code snippets\n",
    "    reference_tokens = reference_code.strip().split()\n",
    "    generated_tokens = generated_code.strip().split()\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothie)\n",
    "    return score\n",
    "\n",
    "# Main function to generate code and calculate BLEU score\n",
    "def main(question):\n",
    "    # Generate code using the model\n",
    "    generated_code = generate_code_from_question(question)\n",
    "    \n",
    "    # Retrieve the reference code for the given question\n",
    "    reference_code = reference_codes.get(question, None)\n",
    "    \n",
    "    if reference_code:\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = calculate_bleu_score(reference_code, generated_code)\n",
    "        \n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Generated Code: {generated_code}\")\n",
    "        print(f\"Reference Code: {reference_code}\")\n",
    "        print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    else:\n",
    "        print(f\"No reference code found for the question: {question}\")\n",
    "\n",
    "# Example usage\n",
    "question = \"Function for nth Fibonacci number\"\n",
    "main(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
